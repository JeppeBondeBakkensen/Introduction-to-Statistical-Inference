<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Data Analysis and Statistical Inference | Applied Linear Models</title>
  <meta name="description" content="These are a collection of notes related to STAT 500 at Iowa State University. This is a work in progress." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Data Analysis and Statistical Inference | Applied Linear Models" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are a collection of notes related to STAT 500 at Iowa State University. This is a work in progress." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Data Analysis and Statistical Inference | Applied Linear Models" />
  
  <meta name="twitter:description" content="These are a collection of notes related to STAT 500 at Iowa State University. This is a work in progress." />
  

<meta name="author" content="Nick Syring" />


<meta name="date" content="2022-01-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="cross.html"/>
<script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a></li>
<li class="chapter" data-level="2" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html"><i class="fa fa-check"></i><b>2</b> Data Analysis and Statistical Inference</a><ul>
<li class="chapter" data-level="2.1" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#data-experiments-and-studies"><i class="fa fa-check"></i><b>2.1</b> Data, Experiments, and Studies</a><ul>
<li class="chapter" data-level="2.1.1" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#james-linds-scurvy-trial"><i class="fa fa-check"></i><b>2.1.1</b> James Lind's Scurvy Trial</a></li>
<li class="chapter" data-level="2.1.2" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#framingham-heart-study"><i class="fa fa-check"></i><b>2.1.2</b> Framingham Heart Study</a></li>
<li class="chapter" data-level="2.1.3" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#harris-bank-sex-pay-study"><i class="fa fa-check"></i><b>2.1.3</b> Harris Bank Sex Pay Study</a></li>
<li class="chapter" data-level="2.1.4" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#study-concepts"><i class="fa fa-check"></i><b>2.1.4</b> Study Concepts</a></li>
<li class="chapter" data-level="2.1.5" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#randomization-control-and-causation"><i class="fa fa-check"></i><b>2.1.5</b> Randomization, control, and causation</a></li>
<li class="chapter" data-level="2.1.6" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#populations-and-scope-of-inference"><i class="fa fa-check"></i><b>2.1.6</b> Populations and scope of inference</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#statistical-inference-in-randomly-sampled-randomized-experiments"><i class="fa fa-check"></i><b>2.2</b> Statistical Inference in randomly sampled, randomized experiments</a><ul>
<li class="chapter" data-level="2.2.1" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#one--and-two-sample-inference-for-a-population-mean"><i class="fa fa-check"></i><b>2.2.1</b> One- and Two-sample inference for a population mean</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#exercises"><i class="fa fa-check"></i><b>2.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="cross.html"><a href="cross.html"><i class="fa fa-check"></i><b>3</b> Cross-references</a><ul>
<li class="chapter" data-level="3.1" data-path="cross.html"><a href="cross.html#chapters-and-sub-chapters"><i class="fa fa-check"></i><b>3.1</b> Chapters and sub-chapters</a></li>
<li class="chapter" data-level="3.2" data-path="cross.html"><a href="cross.html#captioned-figures-and-tables"><i class="fa fa-check"></i><b>3.2</b> Captioned figures and tables</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="parts.html"><a href="parts.html"><i class="fa fa-check"></i><b>4</b> Parts</a></li>
<li class="chapter" data-level="5" data-path="footnotes-and-citations.html"><a href="footnotes-and-citations.html"><i class="fa fa-check"></i><b>5</b> Footnotes and citations</a><ul>
<li class="chapter" data-level="5.1" data-path="footnotes-and-citations.html"><a href="footnotes-and-citations.html#footnotes"><i class="fa fa-check"></i><b>5.1</b> Footnotes</a></li>
<li class="chapter" data-level="5.2" data-path="footnotes-and-citations.html"><a href="footnotes-and-citations.html#citations"><i class="fa fa-check"></i><b>5.2</b> Citations</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="blocks.html"><a href="blocks.html"><i class="fa fa-check"></i><b>6</b> Blocks</a><ul>
<li class="chapter" data-level="6.1" data-path="blocks.html"><a href="blocks.html#equations"><i class="fa fa-check"></i><b>6.1</b> Equations</a></li>
<li class="chapter" data-level="6.2" data-path="blocks.html"><a href="blocks.html#theorems-and-proofs"><i class="fa fa-check"></i><b>6.2</b> Theorems and proofs</a></li>
<li class="chapter" data-level="6.3" data-path="blocks.html"><a href="blocks.html#callout-blocks"><i class="fa fa-check"></i><b>6.3</b> Callout blocks</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="sharing-your-book.html"><a href="sharing-your-book.html"><i class="fa fa-check"></i><b>7</b> Sharing your book</a><ul>
<li class="chapter" data-level="7.1" data-path="sharing-your-book.html"><a href="sharing-your-book.html#publishing"><i class="fa fa-check"></i><b>7.1</b> Publishing</a></li>
<li class="chapter" data-level="7.2" data-path="sharing-your-book.html"><a href="sharing-your-book.html#pages"><i class="fa fa-check"></i><b>7.2</b> 404 pages</a></li>
<li class="chapter" data-level="7.3" data-path="sharing-your-book.html"><a href="sharing-your-book.html#metadata-for-sharing"><i class="fa fa-check"></i><b>7.3</b> Metadata for sharing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied Linear Models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="data-analysis-and-statistical-inference" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Data Analysis and Statistical Inference</h1>
<p>In this first chapter we define and discuss some important concepts regarding data and scientific investigation.</p>
<div id="data-experiments-and-studies" class="section level2">
<h2><span class="header-section-number">2.1</span> Data, Experiments, and Studies</h2>
<p>We encounter numerical summaries of information constantly in our everyday lives and sort through these in order to make all sorts of decisions. In this section we will formalize the concept of <em>data</em> connected to scientific study.</p>
<p>Two types of studies in which data are collected and analyzed are in designed, interventional experiments, and in observational studies. The following illustrations help to differentiate these two types of studies.</p>
<div id="james-linds-scurvy-trial" class="section level3">
<h3><span class="header-section-number">2.1.1</span> James Lind's Scurvy Trial</h3>
<p>Clinical trials are familiar designed, interventional experiments. A famous, early example is James Lind's Scurvy trial. Lind was a doctor aboard a British ship. Several sailors with him were suffering from scurvy. He selected 12 sailors in similar, poor condition, and assigned to pairs of them 6 different treatments (the intervention). The two who received oranges and lemons to eat recovered fully; those who received apple cider fared next best.</p>
</div>
<div id="framingham-heart-study" class="section level3">
<h3><span class="header-section-number">2.1.2</span> Framingham Heart Study</h3>
<p>Named for Framingham, Massachusetts, where the participants were first recruited, is a long-running observational study of Americans aimed at understanding risks associated with heart disease. Participants agreed to medical testing every 3-5 years, from which the study researchers concluded a number of important findings, such as cigarett smoking substantially increases the risk of heart disease. There are no interventions; the researchers simply observe the patients and make conclusions based on how the patients choose to live, e.g., tobacco use.</p>
</div>
<div id="harris-bank-sex-pay-study" class="section level3">
<h3><span class="header-section-number">2.1.3</span> Harris Bank Sex Pay Study</h3>
<p>93 salaries of entry-level clerical workers who started working at Harris Bank between 1969 and 1971 show men were paid more than women. (From The Statistical Sleuth, reproduced from “Harris Trust and Savings Bank: An Analysis of Employee Compensation” (1979), Report 7946,Center for Mathematical Studies in Business and Economics, University of Chicago Graduate School of Business.)</p>
</div>
<div id="study-concepts" class="section level3">
<h3><span class="header-section-number">2.1.4</span> Study Concepts</h3>
<p>The above examples illustrate several key concepts related to scientific studies.</p>
<ul>
<li>Research question - There is always a reason researchers go to the trouble of collecting and analysing data; they have an important question they want to answer. For example, what can sailors do to prevent scurvy?</li>
<li>Experimental units/Subjects - the research question usually references people, things, animals, or some other entity that can be studied in order to answer the question. When these are observed and measured then they are called experimental units or subjects.<br />
</li>
<li>Data - we are inundated with information, numbers, figures, and graphs in our everyday lives. Is this data? Anything information gathered to answer a particular research question can be considered data. Relevancy to a research question is key.<br />
</li>
<li>Intervention - When James Lind gave different foods to sick sailors he was making an intervention, and his goal was to study the effect of his interventions on the sailors well-being. Experiments include one or more interventions, whereas observational studies do not feature any interventions on the part of the researcher.</li>
<li>Randomization - When researchers intervene, they should apply their interventions randomly with respect to subjects. In experiments the experimental units are the entities that are randomized and given interventions.</li>
<li>Response/outcome variables - studies often measure multiple variables and study relationships between them. Typically the researchers expect one variable is affected by another. The response, or outcome---like the health of sailors, or pay of workers---is the variable beign effected by the intervention in an experiment or by another, independent variable in an observational study.</li>
<li>Control - Researchers should try to limit the effects of variables on the response that are not of interest to the study. For example, in the gender pay study, the researchers studied only entry-level workers. They <em>controlled</em> for prior experience to better isolate potential sex effects on pay.</li>
</ul>
</div>
<div id="randomization-control-and-causation" class="section level3">
<h3><span class="header-section-number">2.1.5</span> Randomization, control, and causation</h3>
<p>In experiments the researcher performs one or more interventions---such as giving patients cider versus citrus fruits in Lind's scurvy trial. The principle of {} asserts that interventions in experiments should be assigned to experimental units randomly. When experimental units are heterogeneous---not all the same---it stands to reason that some of their differences apart from the intervention may impact the response to the experiment recorded by the researcher. Randomization is a way to even out these heterogeneities between groups receiving different interventions. That way, it is the intervention, rather than some other difference, which is responsible for substantially different outcomes. Randomization systematically accounts for heterogeneities, but the extent to which it works depends on the number of experimental units, the number of groups being randomized, and the presence of one or more important heterogeneities. For examples, consider the following: 1. Suppose in ten experimental units there is one unobserved, dichotomous trait that affects the experimental response. Three of the ten have version &quot;0&quot; of the trait and 7 have version &quot;1&quot;. Randomly split the ten into two groups of five, each group to receive a different intervention. The chance all three end up in the same group is 1/6, not ignorably small... 2. On the other hand, suppose there are 100 experimental units, half have trait &quot;0&quot; and half trait &quot;1&quot;. The chance at least 35 of either trait type end up in the same one half random split is <span class="math inline">\(\approx 0\)</span>.<br />
Generally when an intervention is randomized over experimental units we interpret any significant difference in outcome/response between intervenion groups as having been {} by the intervention itself, as opposed to some other unobserved characteristic---these are sometimes called {} or {}. But, randomization is not foolproof; small sample sizes (few experimental units) and/or the presence of many confounding variables can reduce the effectiveness of randomization.</p>
<p>When the researcher knows about potential confounders ahead of time, the principle of <em>blocking</em> says experimental units should be representatively divided with respect to intervention across values of this variable. For example, if experimental units are humans both young and old, then the different interventions should be applied to equal numbers of young and old people. One way to accomplish this is to let age group be a <em>blocking factor</em>. In the case of a dichotomous intervention this means half of the old people will be randomly assigned to one intervention and half of the young people will be randomly assigned to one intervention---as opposed to randomly assigning half of all the experimental units to one intervention.</p>
<p>The principle of <em>control</em> states that intervention groups should be made as homogeneous as possible. When experiments are well-controlled researchers often assume that they can determine <em>causation</em>, and any observed differences in experimental outcome between intervention groups can be attributed to the intervention. Of course, as mentioned above, the ability of an experiment to determine causation is not all or nothing; rather, it depends on unknowns. Nevertheless, stronger controls make the results of experiments more trustworthy, and less likely to be caused by confounding variables.</p>
<p>Non-interventional, observational studies are not used to establish causative relationships. Rather, we say such studies establish <em>associations</em> between variables. For example, in the Framingham study, the researchers did not randomly assign individuals to groups of tobacco-users and non-users. Even though these days the evidence is quite strong that tobacco use causes heart disease, the bar for such a claim is much higher when the variable of interest---tobacco use---cannot be randomly assigned to experimental units. That's not to say elements of control cannot be used. For instance, if enough data is collected, it is possible to compare tobacco-users and non-users with nearly the same ages, sexes, incomes, education, living in the same zip codes, etc. The more potential confounders are explicitly controlled, the closer such an observational study comes to a randomized experiment.</p>
</div>
<div id="populations-and-scope-of-inference" class="section level3">
<h3><span class="header-section-number">2.1.6</span> Populations and scope of inference</h3>
<p>Whether conducting and experiment or collecting observational data, the units/subjects have to come from somewhere. <em>Sampling</em> describes how subjects are obtained for observation. <em>Random sampling</em> is any scheme involving selecting a subset of subjects from a larger group in some random fashion. A <em>simple random sample</em> of size <span class="math inline">\(n\)</span> is obtained when every subset of <span class="math inline">\(n\)</span> subjects from a total group is equally likely to be selected. Other types of random selection are possible, but we won't often consider these: - <em>stratified random sampling</em> obtains when simple random samples from separate groups/strata are combined, e.g., a 50/50 random sample stratified by male/female can be formed by taking a simple random sample of ten males and a simple random sample of 10 females from a group of 50 males and 50 females - <em>cluster random sampling</em> obtains when a larger group is subdivided into smaller groups and subgroups are selected at random, e.g., a cluster random sample of Iowa high schoolers can be obtained by choosing all high schoolers who attend one of a simple random sample of Iowa high schools.</p>
<p>When subjects are randomly sampled from a larger group we call that larger group the <em>population</em>. Generally, conclusions about subjects in the study---whether it is an experiment or an observational study---may be assumed to hold for the wider population as a whole when the subjects are chosen randomly. The details of this <em>generalizability</em> of results depend on the type of random sampling conducted; we'll focus on the case of simple random sampling specifically. On the other hand, when subjects are not randomly sampled from the population, study results cannot be generalized back to the population. The reason is that the lack of randomness in selection implies some subsets of the population are more or less likely to be present in the sample of subjects observed, hence, that sample is not necessarily <em>representative</em> of the population. For an extreme example, consider a population of both young and old people and an experiment studying the effects of Covid-19. It's well-known Covid-19 is much more harmful to old people compared to young people. So this is a potential confounder. If we select only young people to study, then we certainly cannot claim the results would be similar had we studied both young and old people.</p>
<p>Non-random sampling schemes are quite common because they are usually easier and cheaper to implement than random sampling schemes. A <em>convenience sample</em> is just what it sounds like---a rule that selects subjects that are easy to select---such as conducting a poll of your closest friends. When a non-random sample is used, remember that the results cannot be interpreted beyond the group subjects that were observed.</p>
<p>Sometimes a researcher intends to study one population, but possese data from another population. This mismatch is important to identify as it can cause bias---which simply means the answer to the researcher's question is different for the population for which data is observed compared to the intended population. As in the extreme example above, effects of Covid-19 are different in old and young populations, so the results from an experiment studying only the young are biased when viewed from the perspective of the population of old and young combined.</p>
</div>
</div>
<div id="statistical-inference-in-randomly-sampled-randomized-experiments" class="section level2">
<h2><span class="header-section-number">2.2</span> Statistical Inference in randomly sampled, randomized experiments</h2>
<p>In this section we will cover some familiar statistical methods for inference in one- and two-sample problems. <em>Inference</em> refers to generalizing results from a sample of experimental units/subjects to a population while properly accounting for <em>statistical uncertainty</em>. The latter is the variability in the experimental outcome/observation that naturally follows from the fact the subjects were chosen randomly from the population.</p>
<div id="one--and-two-sample-inference-for-a-population-mean" class="section level3">
<h3><span class="header-section-number">2.2.1</span> One- and Two-sample inference for a population mean</h3>
<p>Let <span class="math inline">\(P\)</span> denote a population for which the mean <span class="math inline">\(\theta(P):=E(X), \,X\sim P\)</span> and second moment <span class="math inline">\(E(X^2)\)</span> exist i.e., <span class="math inline">\(E(X^2)&lt;\infty\)</span>. Let <span class="math inline">\(X_1, \ldots, X_n\)</span> denote a simple random sample (independent, and identically distributed collection) from <span class="math inline">\(P\)</span>.</p>
<p>The sample mean <span class="math inline">\(\overline{X}\)</span> is <em>unbiased</em> for <span class="math inline">\(\theta\)</span>, that is, its expectation is <span class="math display">\[E(\overline X) = E\left(\tfrac1n \sum_{i=1}^n X_i\right) = \tfrac1n \sum_{i=1}^n E(X_i) = \tfrac1n \sum_{i=1}^n \theta = \theta\]</span> is equal to <span class="math inline">\(\theta\)</span>.</p>
<p>Since <span class="math inline">\(X\sim P\)</span> has two finite moments it has a finite variance, denoted <span class="math inline">\(\sigma^2\)</span>. The variance of <span class="math inline">\(\overline X\)</span> is <span class="math display">\[V(\overline X) = V\left(\tfrac1n \sum_{i=1}^n X_i\right) = \tfrac{1}{n^2}\left(\sum_{i=1}^n V(X_i) + \sum_{i=1, j&gt;i}^n 2Cov(X_i, X_j)\right)\]</span> <span class="math display">\[ = \tfrac{1}{n^2}\left(\sum_{i=1}^n V(X_i)\right) = \sigma^2/n.\]</span></p>
<p>The central limit theorem (CLT) says that <span class="math display">\[\sqrt n(\overline{X} - \theta) \stackrel{D}{\rightarrow} N(0, \sigma^2)\]</span> where <span class="math inline">\(V(X) =\sigma^2\)</span> and ``<span class="math inline">\(\stackrel{D}{\rightarrow}\)</span>&quot; denotes convergence in distribution. Reminder: convergence in distribution means pointwise convergence of cumulative distribution functions; so the scaled, centered sample mean <span class="math inline">\(\sqrt n(\overline{X} - \theta)\)</span> has a distribution function (that depends on the sample size <span class="math inline">\(n\)</span>) that converges pointwise to the normal distribution function with variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>A <span class="math inline">\(100(1-\alpha)\%\)</span> confidence interval (CI) for <span class="math inline">\(\theta\)</span> is a random interval <span class="math inline">\((\ell, u)\)</span> such that <span class="math display">\[P(\ell \leq \theta \leq u) \geq 1-\alpha.\]</span> Let <span class="math inline">\(z_\alpha\)</span> denote the <span class="math inline">\(\alpha\)</span> quantile of the standard normal distribution, i.e., if <span class="math inline">\(Z\)</span> is a standard normal r.v. then <span class="math inline">\(z_\alpha\)</span> satisfies <span class="math inline">\(P(Z\leq z_\alpha) = \alpha\)</span>. Suppose <span class="math inline">\(\sigma^2\)</span> is known and let <span class="math inline">\(\ell := \overline{X} + z_{\alpha/2}\sigma/\sqrt{n}\)</span> and <span class="math inline">\(u := \overline{X} + z_{1-\alpha/2}\sigma/\sqrt{n}\)</span>. Then, <span class="math inline">\((\ell, u)\)</span> is a <span class="math inline">\(100(1-\alpha)\%\)</span> CI for <span class="math inline">\(\theta\)</span>.</p>
<p>Interpretation of CIs: You likely have heard an interpretation of the ``confidence property&quot; before. It goes something like the following: imagine repeating the random sampling of data from <span class="math inline">\(P\)</span> many, many times, say 1000 times, and for each set of data computing a, say, <span class="math inline">\(95\%\)</span> CI for <span class="math inline">\(\theta\)</span>. Confidence means about 950 of these 1000 intervals contain the true <span class="math inline">\(\theta\)</span>. There's nothing wrong with this understanding, except many people don't understand it. I prefer a simpler explanation: <em>a confidence interval is a set of plausible values for <span class="math inline">\(\theta\)</span></em> where the degree of plausibility obviously is determined by <span class="math inline">\(\alpha\)</span>. This simple explanation is theoretically grounded in <em>possibility theory</em> but a discussion of such topics is, unfortunately, beyond our present scope.</p>
<p>Let <span class="math inline">\(\Theta_0\)</span> be a subset of <span class="math inline">\(\mathbb{R}\)</span>. Define the null hypothesis as the assertion <span class="math inline">\(H_0: \theta\in \Theta_0\)</span>; correspondingly, the alternative hypothesis is <span class="math inline">\(H_a: \theta\in \Theta_0^c\)</span>. A testing rule <span class="math inline">\(g\)</span> says <span class="math inline">\(H_0\)</span> is retained/not rejected if <span class="math inline">\(g&gt;t\)</span> for some value <span class="math inline">\(t\)</span> and is rejected otherwise. There are four possibilities: 1. <span class="math inline">\(H_0\)</span> is true and is retained 2. <span class="math inline">\(H_0\)</span> is false and is rejected 3. <span class="math inline">\(H_0\)</span> is true and is rejected, a Type 1 error 4. <span class="math inline">\(H_0\)</span> is false and is retained, a Type 2 error. Typically, hypotheses are defined such that a Type 1 error is worse than a Type 2 error. Therefore, we try to minimize the chance of committing a Type 1 error.</p>
<p>Define <span class="math display">\[T(\theta_0) = \frac{\overline X - \theta_0}{\sigma/\sqrt{n}}, \quad \theta_0 \in \Theta_0.\]</span> Let <span class="math inline">\(g:= \sup_{\theta_0} P(Z &gt; |T(\theta_0)|)\)</span>. Then, <span class="math inline">\(g\)</span> is <em>p-value</em> with respect to <span class="math inline">\(H_0\)</span> and if we set <span class="math inline">\(t = \alpha\)</span> then the CLT implies (for large <span class="math inline">\(n\)</span>) <span class="math display">\[P(\text{Reject }H_0\text{ when }H_0\text{ true}) \leq 1-\alpha.\]</span> The most common form of this test is the <em>point-null</em> test in which <span class="math inline">\(\Theta_0 = \{\theta_0\}\)</span> is a singleton set (and the supremum in the p-value definition is unnecessary).</p>
</div>
</div>
<div id="exercises" class="section level2">
<h2><span class="header-section-number">2.3</span> Exercises</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="cross.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/01-intro.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
